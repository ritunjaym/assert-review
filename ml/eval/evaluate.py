"""
Evaluation orchestrator: runs baselines + reranker + clustering metrics.
Generates HTML report at ml/eval/report.html.
Logs results to W&B if API key is available.
"""
from __future__ import annotations

import json
import os
from pathlib import Path

import numpy as np

from .metrics import ranking_metrics, clustering_metrics
from .baselines import RandomBaseline, FileSizeBaseline, PathHeuristicBaseline

EVAL_DIR = Path(__file__).parent


def _generate_html_report(results: dict, sample_rankings: list[dict]) -> str:
    """Generate a self-contained HTML report with inline SVG bar charts."""
    
    def bar_chart(metrics: dict[str, float], title: str) -> str:
        bars = ""
        max_val = max((abs(v) for v in metrics.values()), default=1) or 1
        for name, val in metrics.items():
            width = int(abs(val) / max_val * 200)
            color = "#4ade80" if val >= 0 else "#f87171"
            bars += f"""
            <tr>
              <td style="padding:4px;width:120px;font-size:12px">{name}</td>
              <td><svg width="220" height="16">
                <rect x="0" y="2" width="{width}" height="12" fill="{color}" rx="2"/>
                <text x="{width+4}" y="13" font-size="11" fill="#333">{val:.4f}</text>
              </svg></td>
            </tr>"""
        return f"""
        <div style="margin:16px 0">
          <h3 style="font-size:14px;margin:0 0 8px">{title}</h3>
          <table style="border-collapse:collapse">{bars}</table>
        </div>"""
    
    model_sections = ""
    for model_name, metrics in results.items():
        model_sections += bar_chart(metrics, model_name)
    
    sample_html = ""
    for sample in sample_rankings[:5]:
        pr_id = sample.get("pr_id", "?")
        files_html = "".join(
            f"<tr><td style='font-size:11px;padding:2px 8px'>{f['filename']}</td>"
            f"<td style='font-size:11px;color:#666'>{f.get('true_score', 0):.3f}</td>"
            f"<td style='font-size:11px;color:#4ade80'>{f.get('pred_score', 0):.3f}</td></tr>"
            for f in sample.get("files", [])[:8]
        )
        sample_html += f"""
        <div style="margin:16px 0;border:1px solid #e5e7eb;border-radius:8px;padding:12px">
          <h4 style="font-size:13px;margin:0 0 8px">PR #{pr_id}</h4>
          <table style="border-collapse:collapse;width:100%">
            <tr><th style="font-size:11px;text-align:left;padding:2px 8px">File</th>
                <th style="font-size:11px;text-align:left;padding:2px 8px">True</th>
                <th style="font-size:11px;text-align:left;padding:2px 8px">Pred</th></tr>
            {files_html}
          </table>
        </div>"""
    
    return f"""<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Assert Review — ML Evaluation Report</title>
<style>
  body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
         max-width: 900px; margin: 40px auto; padding: 0 24px; color: #1f2937; }}
  h1 {{ font-size: 24px; }}
  h2 {{ font-size: 18px; border-bottom: 1px solid #e5e7eb; padding-bottom: 8px; }}
</style>
</head>
<body>
<h1>Assert Review — ML Evaluation Report</h1>
<p style="color:#6b7280">Generated by ml/eval/evaluate.py</p>

<h2>Ranking Metrics by Model</h2>
{model_sections}

<h2>Sample PR Rankings</h2>
{sample_html}

<h2>Raw Results</h2>
<pre style="background:#f9fafb;padding:16px;border-radius:8px;font-size:12px;overflow:auto">{json.dumps(results, indent=2)}</pre>
</body>
</html>"""


def run_evaluation(
    dataset_path: Path | None = None,
    max_samples: int = 100,
    log_wandb: bool = True,
) -> dict:
    """
    Run all baselines and log results.
    If no dataset is available, uses synthetic data.
    """
    # Load or generate data
    records = _load_records(dataset_path, max_samples)
    
    if not records:
        print("No dataset found. Using synthetic data for evaluation demo.")
        records = _make_synthetic_records(50)
    
    y_true = [r.get("importance_score", 0.5) for r in records]
    
    # Run baselines
    baselines = {
        "RandomBaseline": RandomBaseline(),
        "FileSizeBaseline": FileSizeBaseline(),
        "PathHeuristicBaseline": PathHeuristicBaseline(),
    }
    
    results = {}
    
    for name, baseline in baselines.items():
        ranked = baseline.rank(records)
        y_pred = [r.get("baseline_score", 0.5) for r in ranked]
        # Re-align to original order
        score_map = {r.get("filename", str(i)): r.get("baseline_score", 0.5) 
                    for i, r in enumerate(ranked)}
        y_pred_aligned = [score_map.get(r.get("filename", str(i)), 0.5) 
                         for i, r in enumerate(records)]
        results[name] = ranking_metrics(y_true, y_pred_aligned)
    
    # Try reranker
    try:
        from ml.models.reranker import Reranker
        reranker = Reranker()
        texts = [
            f"<file>{r.get('filename','')}</file>"
            f"<diff>{(r.get('patch','') or '')[:512]}</diff>"
            for r in records
        ]
        scores = reranker.score(texts)
        results["Reranker"] = ranking_metrics(y_true, scores)
    except Exception as e:
        print(f"Reranker evaluation skipped: {e}")
    
    # Sample rankings for report
    sample_rankings = []
    if records:
        sample = records[:20]
        baseline_ranked = PathHeuristicBaseline().rank(sample)
        pred_scores = {r.get("filename"): r.get("baseline_score") for r in baseline_ranked}
        sample_rankings.append({
            "pr_id": "sample",
            "files": [
                {
                    "filename": r.get("filename", ""),
                    "true_score": r.get("importance_score", 0),
                    "pred_score": pred_scores.get(r.get("filename"), 0),
                }
                for r in sample[:8]
            ]
        })
    
    # Generate HTML report
    report_html = _generate_html_report(results, sample_rankings)
    report_path = EVAL_DIR / "report.html"
    report_path.write_text(report_html)
    print(f"Report saved to {report_path}")
    
    # Log to W&B
    if log_wandb:
        _log_wandb(results)
    
    # Print summary table
    print("\n=== Evaluation Results ===")
    for model, metrics in results.items():
        spearman = metrics.get("spearman_rho", 0)
        mse = metrics.get("mse", 0)
        print(f"  {model:30s} | Spearman rho: {spearman:+.4f} | MSE: {mse:.4f}")
    
    return results


def _load_records(dataset_path: Path | None, max_samples: int) -> list[dict]:
    """Load records from JSONL file or HF dataset."""
    paths_to_try = []
    if dataset_path:
        paths_to_try.append(dataset_path)
    
    base = Path(__file__).parent.parent
    paths_to_try.extend([
        base / "data" / "processed" / "pr_files.jsonl",
        base / "data" / "processed" / "pr_hunks.jsonl",
    ])
    
    for path in paths_to_try:
        if path and path.exists():
            records = []
            with open(path) as f:
                for line in f:
                    if line.strip():
                        records.append(json.loads(line))
                        if len(records) >= max_samples:
                            break
            return records
    
    # Try HF dataset
    hf_path = base / "data" / "hf_dataset"
    if hf_path.exists():
        try:
            from datasets import load_from_disk
            ds = load_from_disk(str(hf_path))
            test_split = ds.get("test") or ds.get("train")
            if test_split:
                return [test_split[i] for i in range(min(max_samples, len(test_split)))]
        except Exception:
            pass
    
    return []


def _make_synthetic_records(n: int) -> list[dict]:
    """Generate synthetic records for evaluation without real data."""
    import math
    rng = __import__("random").Random(42)
    
    filenames = [
        "src/auth/login.py", "src/ui/button.tsx", "tests/test_auth.py",
        "README.md", "src/api/routes.py", "package.json", "src/core/engine.py",
        "docs/guide.md", "src/crypto/hash.py", "config/settings.yaml",
    ]
    
    records = []
    for i in range(n):
        filename = filenames[i % len(filenames)]
        additions = rng.randint(1, 200)
        deletions = rng.randint(0, 50)
        is_security = any(kw in filename for kw in ["auth", "crypto", "login"])
        importance = min(1.0, 0.3 + (0.4 if is_security else 0) + additions / 500)
        records.append({
            "filename": filename,
            "additions": additions,
            "deletions": deletions,
            "importance_score": importance,
            "patch": f"+code_line_{i}" * min(10, additions),
        })
    return records


def _log_wandb(results: dict) -> None:
    try:
        import wandb
        if os.environ.get("WANDB_API_KEY"):
            wandb.init(
                project=os.environ.get("WANDB_PROJECT", "assert-review"),
                job_type="evaluation",
            )
            flat = {}
            for model, metrics in results.items():
                for k, v in metrics.items():
                    flat[f"{model}/{k}"] = v
            wandb.log(flat)
            wandb.finish()
    except Exception:
        pass


if __name__ == "__main__":
    run_evaluation()
